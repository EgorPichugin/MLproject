{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Evaluation\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import MulticlassROC, MulticlassAUROC\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "\n",
    "# Settings\n",
    "MODEL_NAME = \"\" # Specify model before run\n",
    "num_classes = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save loss/acc graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_arrays = pd.read_csv(\"visualisation_arrays.csv\")\n",
    "\n",
    "# Convert to lists\n",
    "train_loss_array = visualisation_arrays[\"Train loss\"].tolist()\n",
    "test_loss_array = visualisation_arrays[\"Test loss\"].tolist()\n",
    "train_acc_array = visualisation_arrays[\"Train acc\"].tolist()\n",
    "test_acc_array = visualisation_arrays[\"Test acc\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save loss plot\n",
    "plt.plot(train_loss_array, label=\"Train Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\", markersize=6, linewidth=2)\n",
    "plt.plot(test_loss_array, label=\"Test Loss\", linestyle=\"--\", color=\"orange\", marker=\"s\", markersize=6, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Loss Graph: v{MODEL_NAME}\", weight=\"bold\")\n",
    "plt.legend(loc=\"upper right\", fontsize=12, frameon=True, shadow=True, fancybox=True, borderpad=1)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.7)\n",
    "plt.savefig(f\"loss_graph_v{MODEL_NAME}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save acc plot\n",
    "plt.plot(train_acc_array, label=\"Train Accuracy\", linestyle=\"-\", color=\"blue\", marker=\"o\", markersize=6, linewidth=2)\n",
    "plt.plot(test_acc_array, label=\"Test Accuracy\", linestyle=\"--\", color=\"orange\", marker=\"s\", markersize=6, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"Accuracy Graph: {MODEL_NAME}\", weight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=12, frameon=True, shadow=True, fancybox=True, borderpad=1)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.7)\n",
    "plt.savefig(f\"acc_graph_{MODEL_NAME}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pt for visualisation\n",
    "loaded_data = torch.load(\"visualisation_tensors.pt\")\n",
    "\n",
    "# Convert to tensors\n",
    "all_probabilities = loaded_data[\"all_probabilities\"]\n",
    "all_predictions = loaded_data[\"all_predictions\"]\n",
    "all_labels = loaded_data[\"all_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Roc curve\n",
    "roc_metric = MulticlassROC(num_classes=num_classes).to(device)\n",
    "auc_metric = MulticlassAUROC(num_classes=num_classes, average=None).to(device)\n",
    "\n",
    "colors = [\"blue\", \"yellow\", \"green\", \"red\"]\n",
    "\n",
    "fpr, tpr, _ = roc_metric(all_probabilities, all_labels)\n",
    "auc_score = auc_metric(all_probabilities, all_labels)\n",
    "\n",
    "for i, _ in enumerate(class_names):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC: {auc_score[i]})\", color=colors[i], markersize=6, linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve: {MODEL_NAME}\", weight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=12, frameon=True, shadow=True, fancybox=True, borderpad=1)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.7)\n",
    "plt.savefig(f\"roc_curve_{MODEL_NAME}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Confusion Matrix\n",
    "conf_matrix_metric = MulticlassConfusionMatrix(num_classes=num_classes).to(device)\n",
    "\n",
    "conf_matrix = conf_matrix_metric(all_predictions, all_labels).cpu().numpy()\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"mako_r\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(f\"Confusion Matrix: {MODEL_NAME}\", weight=\"bold\")\n",
    "plt.savefig(f\"conf_matrix_{MODEL_NAME}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision for each class\n",
    "true_positive = np.diag(conf_matrix)\n",
    "false_positive = np.sum(conf_matrix, axis=0) - true_positive\n",
    "precision_per_class = true_positive / (true_positive + false_positive)\n",
    "\n",
    "# Macro Precision - average precision across all classes\n",
    "macro_precision = np.mean(precision_per_class)\n",
    "\n",
    "# Micro Precision - global precision across all classes\n",
    "total_true_positive = np.sum(true_positive)\n",
    "total_false_positive = np.sum(false_positive)\n",
    "micro_precision = total_true_positive / (total_true_positive + total_false_positive)\n",
    "\n",
    "precision_per_class, macro_precision, micro_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TP, FP, FN, TN\n",
    "true_prediction = np.diag(conf_matrix)\n",
    "false_prediction = np.sum(conf_matrix, axis=0) - true_prediction\n",
    "false_negative = np.sum(conf_matrix, axis=1) - true_prediction\n",
    "true_negative = np.sum(conf_matrix) - (true_prediction + false_prediction + false_negative)\n",
    "\n",
    "# Calculate Precision\n",
    "precision_per_class = true_prediction / (true_prediction + false_prediction)\n",
    "macro_precision = np.mean(precision_per_class)\n",
    "micro_precision = np.sum(true_prediction) / (np.sum(true_prediction) + np.sum(false_prediction))\n",
    "\n",
    "# Calculate Recall (Sensitivity)\n",
    "recall_per_class = true_prediction / (true_prediction + false_negative)\n",
    "macro_recall = np.mean(recall_per_class)\n",
    "micro_recall = np.sum(true_prediction) / (np.sum(true_prediction) + np.sum(false_negative))\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity_per_class = true_negative / (true_negative + false_prediction)\n",
    "macro_specificity = np.mean(specificity_per_class)\n",
    "micro_specificity = np.sum(true_negative) / (np.sum(true_negative) + np.sum(false_prediction))\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_per_class = 2 * (precision_per_class * recall_per_class) / (precision_per_class + recall_per_class)\n",
    "macro_f1 = np.mean(f1_per_class)\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "\n",
    "# Display results\n",
    "precision_per_class, macro_precision, micro_precision, recall_per_class, macro_recall, micro_recall, specificity_per_class, macro_specificity, micro_specificity, f1_per_class, macro_f1, micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = [\"Healthy\", \"Stage 1\", \"Stage 2\",\"Stage 3\"]\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision_per_class,\n",
    "    \"Recall\": recall_per_class,\n",
    "    \"Specificity\": specificity_per_class,\n",
    "    \"F1-Score\": f1_per_class\n",
    "})\n",
    "print(metrics_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics_df.set_index(\"Class\").T.plot(kind='bar', ax=ax)\n",
    "plt.title(\"Classification Metrics per Class\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for the metrics\n",
    "categories = [\"Healthy\", \"Stage 1\", \"Stage 2\",\"Stage 3\"]\n",
    "N = len(categories)\n",
    "\n",
    "# Define values for each class\n",
    "values = {\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision_per_class,\n",
    "    \"Recall\": recall_per_class,\n",
    "    \"Specificity\": specificity_per_class,\n",
    "    \"F1-Score\": f1_per_class\n",
    "}\n",
    "pd.DataFrame({\n",
    "    \n",
    "})\n",
    "\n",
    "# Set up the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Close the radar chart loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each class\n",
    "for class_name, vals in values.items():\n",
    "    vals += vals[:1]  # Close the loop for each metric\n",
    "    ax.plot(angles, vals, label=class_name, linewidth=2)\n",
    "    ax.fill(angles, vals, alpha=0.1)\n",
    "\n",
    "# Format the chart\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12)\n",
    "ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=10)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.title(\"Radar Chart of Classification Metrics\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
